<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=1050, initial-scale=0.3, minimum-scale=0.1, maximum-scale=3.0, user-scalable=yes">
  <title>Researcher · Artificial Intelligence</title>
  <meta name="description" content="Researcher in AI, publications, software, and contact." />
  <style>
    :root {
      --bg:#f7f7f7; --paper:#fff; --ink:#222; --muted:#6a6a6a; --rule:#d9e2ef;
      --link:#0a58ca; --nav1:#eef5ff; --nav2:#dbeaff; --shadow:rgba(0,0,0,.08);
      --rad:14px; --maxw:1050px; --g:22px; --portrait-w:220px;
    }
    * { box-sizing:border-box; margin:0; padding:0 }
    body { font:16px/1.6 Georgia,"Times New Roman",serif; color:var(--ink); background:var(--bg); padding-bottom:40px }
    html, body {overflow-x: auto; }
    img { max-width:100%; display:block }
    a { color:var(--link); text-decoration:none }
    a:hover { text-decoration:underline }

    .wrap { width:var(--maxw); margin:28px auto; padding:0 }
    .card { background:var(--paper); border:1px solid var(--rule); border-radius:var(--rad); box-shadow:0 12px 34px var(--shadow); width: 1050px; margin:0 auto; }

    header { background:linear-gradient(#fff,#f2f6ff); padding:20px 20px 60px; position:relative }
    .mast { display:flex; gap:16px; align-items:center }
    .mast h1 { font-size:32px }
    .tagline { margin-top:2px; color:var(--muted); font-size:18px }

    nav { position:absolute; bottom:0; left:20px; right:20px; border-bottom:1px solid var(--rule) }
    .tabs { display:flex; gap:10px; flex-wrap:wrap; align-items:flex-end }
    .tab { padding:10px 18px; border:1px solid #c9d7ec; border-bottom:none; border-radius:12px 12px 0 0;
           background:linear-gradient(var(--nav1),var(--nav2)); font:14px/1.1 Verdana,sans-serif; color:#2a3a4a }
    .tab.active { background:#fff; font-weight:600 }

    .shell { padding:var(--g) }
    .grid { display:grid; grid-template-columns:2fr 1fr; gap:var(--g); width: 100%;  }

    h2 { font-size:22px; margin-bottom:10px; border-bottom:1px solid var(--rule); padding-bottom:6px }
    section+section { margin-top:var(--g) }
    ul { padding-left:22px }

    .about-grid { display:grid; grid-template-columns:240px 1fr; gap:16px; align-items:start }
    .lead { font-size:16px; }

    .about-media { width:var(--portrait-w) }
    .portrait { width:var(--portrait-w); border-radius:8px; border:1px solid var(--rule); box-shadow:0 2px 8px var(--shadow); margin-bottom:12px }

    .chips { display:flex; flex-wrap:wrap; gap:8px; margin:16px 0 }
    .chip { padding:4px 10px; border:1px solid #cbd5e1; border-radius:999px; font:12px Verdana,sans-serif;
            background:linear-gradient(#fdfdfd,#f2f6ff); color:#314154; box-shadow:inset 0 1px 0 #fff }

    .btn { display:inline-block; padding:10px 16px; border-radius:10px; border:1px solid #b6c6e3;
           background:linear-gradient(#fff,#e9f1ff); font:14px Verdana,sans-serif; color:#1e3a8a;
           box-shadow:inset 0 1px 0 #fff,0 2px 4px rgba(0,0,0,.06); margin-top: 10px; }
    .btn:hover { filter:brightness(1.03) }
    .btn-block { display:block; width:var(--portrait-w); text-align:center; margin-top:8px }

    .box { background:#fbfdff; border:1px solid var(--rule); border-radius:12px; padding:14px; margin-bottom:14px }
    .contact { list-style:none; font:14px Verdana,sans-serif; padding:0 }
    .contact li { margin:6px 0 }

    .date { color:var(--ink); font:12px Verdana,sans-serif; font-weight:bold }
    .desc { margin-top:4px; font-size:14px; color:#444; font-style:italic; }
    .stacked li+li { margin-top:14px }
    .thumb { border-radius:8px; border:1px solid var(--rule); margin-bottom:10px }
    hr.sep {border: 0; border-top: 4px dotted var(--rule); }

    footer { padding:16px; border-top:1px solid var(--rule); color:var(--muted); font-size:14px; text-align:center; margin:28px 0 }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <header>
        <div class="mast">
          <div>
            <h1>Federico Chinello</h1>
            <p class="tagline">Researcher · Artificial Intelligence</p>
          </div>
        </div>
        <nav>
          <div class="tabs">
            <a class="tab active" href="#about">About</a>
            <a class="tab" href="#news">News &amp; Highlights</a>
            <a class="tab" href="#publications">Publications</a>
            <a class="tab" href="#software">Software</a>
            <a class="tab" href="#results">Selected Results</a>
          </div>
        </nav>
      </header>

      <div class="shell grid">
        <div>
          <section id="about">
            <h2>About</h2>
            <div class="about-grid">
              <div class="about-media">
                <img class="portrait" src="assets/photo.jpeg" alt="Firstname Lastname portrait">
                <a class="btn btn-block" href="cv.pdf" download>⬇️ Download CV (PDF)</a>
              </div>
              <div>
                <p class="lead">
                  I am currently completing an MSc in Artificial Intelligence at Bocconi University. Beginning in October, I will join <a href="https://www.ifom.eu/en/cancer-research/research-labs/research-lab-doksani.php">Ylli Doksani’s lab</a> at the <strong>AIRC Institute of Molecular Oncology (IFOM)</strong> as a <em>Research Fellow</em>, where I will develop computer vision and AI solutions to automate the analysis of microscopy images. I am also part of the Computational Biology group led by <a href="https://www.oncology.ox.ac.uk/team/francesca-buffa">Prof. Francesca Buffa</a> at Bocconi, and I am conducting my master’s thesis on Convolutional Set Transformers—a novel neural architecture we introduced <a href="https://www.arxiv.org/abs/2509.22889">here</a>—under the supervision of <a href="https://boracchi.faculty.polimi.it">Prof. Giacomo Boracchi</a> (Politecnico di Milano). My research interests focus on <strong>deep learning architectures and methods</strong>, with particular emphasis on applications in <strong>computer vision</strong> and <strong>bioinformatics</strong>.
                <div class="chips">
                  <span class="chip">Artificial Intelligence</span>
                  <span class="chip">Deep Learning</span>
                  <span class="chip">Machine Learning</span>
                  <span class="chip">Computer Vision</span>
                  <span class="chip">Bioinformatics</span>
                </div>
              </div>
            </div>
          </section>

          <section id="news">
            <h2>News &amp; Highlights</h2>
            <ul>
              <li><span class="date"><strong>Sep 2025:</strong></span> The <em>Convolutional Set Transformer</em> preprint is available on ArXiv!</li>
              <li><span class="date"><strong>Sep 2025:</strong></span> <strong>I'm looking for a PhD position in AI/ML! 🚨</strong></li>
            </ul>
          </section>

          <section id="publications">
            <h2>Publications</h2>
            <ul class="stacked">
              <li>
                <strong>Convolutional Set Transformer</strong> [ <a href="https://www.arxiv.org/abs/2509.22889">pdf</a> · <a href="https://github.com/chinefed/convolutional-set-transformer">code</a> ]<br>
                Chinello, F., & Boracchi, G., arXiv preprint, 2025. 
                <p class="desc">We introduce a novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semantics (such as a common category, scene, or concept).</p>
              </li>
            </ul>
          </section>

          <section id="software">
            <h2>Software</h2>
            <ul class="stacked">
              <li>
                <strong>cstmodels</strong> [ <a href="https://pypi.org/project/cstmodels/">PyPI</a> · <a href="https://github.com/chinefed/convolutional-set-transformer">code</a> ]<br>
                Chinello, F., 2025. 
                <p class="desc">This package, available on PyPI, provides the reference implementation of the Convolutional Set Transformer. It includes reusable Keras 3 layers for building CST architectures, and provides an easy interface to load and use the CST-15 model pre-trained on ImageNet.</p>
              </li>
            </ul>
          </section>

        <section id="results">
          <h2>Selected Results</h2>
            <img src="assets/graphical_abstract.png" alt="Graphical Abstract CST">
            <br><hr class="sep"><br>
            <p class="desc">
            <strong>Set Anomaly Detection is a binary classification task meant to identify images in a set that are anomalous or inconsistent with the majority of the set.</strong> Here, the notion of anomaly is relative: the same image may be considered anomalous in one set but not in another, depending on the surrounding context. The Figure below shows two image sets derived from the CelebA dataset (Liu et al., 2015). In each set, a majority of normal images share two attributes ("wearing hat" and "smiling" in the first, "no beard" and "attractive" in the second), while a minority lack these attributes and are thus anomalous. After training a CST and a Set Trasnsformer (Lee et al., 2019) on CelebA for Set Anomaly Detection, we evaluate the explainability of their predictions by overlaying Grad-CAMs on anomalous images. CST explanations correctly highlight the anomalous regions, whereas ST explanations fail to provide meaningful insights. For more details see Chinello & Boracchi (2025).
            </p>
            <img src="assets/gradcam_anomaly.png" alt="Set Anomaly Detection">
            <br><hr class="sep"><br>
            <p class="desc">
            <strong>The Figure below presents a qualitative comparison of Grad-CAMs generated for our CST-15 model (28M params), ConvNeXt-XL (350M params), ResNet50 (26M params), and VGG-19 (144M params).</strong> To ensure a fair comparison, we feed isolated images without context to CST-15. Explanations are computed with respect to the ground truth class. CST-15 Grad-CAMs are more precise and focused compared to ResNet50 and VGG-19, and comparable or even better than ConvNeXt-XL explanations. Consider, for instance, the first image (first row in the Figure below). It depicts a space shuttle being transported by a shuttle carrier aircraft. Even to the human eye, it is difficult to distinguish the shuttle from the carrier aircraft. However, the CST-15 explanation map accurately identifies the space shuttle, distinguishing it from the aircraft. In contrast, the Grad-CAMs generated for the other models are significantly less precise, highlighting a coarse region that encompasses both the shuttle and the carrier aircraft. For more details see Chinello & Boracchi (2025).
            </p><br>
            <img src="assets/cst15_gradcam_comparison.png" alt="Grad-CAM: CST-15 vs CNNs ">
          </section>
        </div>

        <aside>
          <div class="box">
            <h2>Contact &amp; Links</h2>
            <ul class="contact">
              <li>✉️ <a href="mailto:federico.chinello@studbocconi.it">federico.chinello@studbocconi.it</a></li>
              <li>💻 <a href="https://github.com/chinefed">GitHub</a></li>
            </ul>
          </div>

          <div class="box">
            <h2>Download CST-15</h2>
            <img class="thumb" src="assets/cst15.png" alt="CST-15 illustration">
            <p class="desc">We introduce CST-15, a CST with 28M parameters trained on ImageNet. CST-15 is the first set-learning backbone pre-trained on a large-scale dataset. It can be adapted to diverse downstream tasks via Transfer Learning. Notably, CSTs fully support standard CNN explainability tools. The Figure above shows Grad-CAM overlays for an image set provided as input to CST-15, with respect to the ground-truth class. CST-15 is readily available through our <strong>cstmodels</strong> package (<code>pip install cstmodels</code>), which also offers reusable Keras 3 layers for building custom CST architectures from scratch.
            <a class="btn" href="https://github.com/chinefed/convolutional-set-transformer">Source code</a>&ensp;
            <a class="btn" href="https://pypi.org/project/cstmodels/">View on PyPI</a>
          </div>
        </aside>
      </div>

      <footer>
        <center>
          © <span id="year"></span> Federico Chinello
        </center>
      </footer>
    </div>
  </div>

  <script>
    (function(){
      var y=document.getElementById('year');
      if(y) y.textContent=new Date().getFullYear();
      function setActive(){
        var hash=location.hash||'#about';
        document.querySelectorAll('.tab').forEach(function(a){
          a.classList.toggle('active',a.getAttribute('href')===hash);
        });
      }
      window.addEventListener('hashchange',setActive);
      setActive();
    })();
  </script>
</body>
</html>